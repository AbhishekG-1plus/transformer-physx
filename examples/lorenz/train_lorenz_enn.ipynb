{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_lorenz_enn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPjTnCG-v2Z6",
        "outputId": "7cb1b531-bb8c-4814-ed89-0001816e7160"
      },
      "source": [
        "\"\"\"\n",
        "Notebook for training the embedding model for the Lorenz system.\n",
        "=====\n",
        "Distributed by: Notre Dame SCAI Lab (MIT Liscense)\n",
        "- Associated publication:\n",
        "url: https://arxiv.org/abs/2010.03957\n",
        "doi: \n",
        "github: https://github.com/zabaras/transformer-physx\n",
        "=====\n",
        "\"\"\"\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun 28 17:04:28 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    28W /  70W |   1192MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34XMtg9FZFql"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvm518_H3AK7",
        "outputId": "1ed0e4ee-dcf9-4bff-a37f-6f64766d34ed"
      },
      "source": [
        "!pip install torch==1.8.1\n",
        "!pip install h5py==2.10.0\n",
        "!pip install filelock==3.0.12\n",
        "!pip install scipy==1.6.3\n",
        "!pip install matplotlib==3.4.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (1.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (3.0.12)\n",
            "Requirement already satisfied: scipy==1.6.3 in /usr/local/lib/python3.7/dist-packages (1.6.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy==1.6.3) (1.19.5)\n",
            "Requirement already satisfied: matplotlib==3.4.2 in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2) (1.3.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.4.2) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib==3.4.2) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoBoGx0J0LtZ"
      },
      "source": [
        "First mount google drive and clone transformer physx repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K0hSst0b2Ak",
        "outputId": "f2287068-fa4b-4234-b2bf-0451fdfe6041"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJL720VLw46q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc294f6b-0b54-4499-c996-d15beb480ca6"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zaXL-m8xEx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf12eec-5af7-48c5-e9c1-35c08ac03c6a"
      },
      "source": [
        "!git clone https://github.com/zabaras/transformer-physx.git\n",
        "%cd ./transformer-physx/examples/lorenz/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformer-physx'...\n",
            "remote: Enumerating objects: 653, done.\u001b[K\n",
            "remote: Counting objects: 100% (653/653), done.\u001b[K\n",
            "remote: Compressing objects: 100% (455/455), done.\u001b[K\n",
            "remote: Total 653 (delta 274), reused 534 (delta 162), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (653/653), 14.68 MiB | 20.13 MiB/s, done.\n",
            "Resolving deltas: 100% (274/274), done.\n",
            "/content/gdrive/My Drive/transformer-physx/examples/lorenz/transformer-physx/examples/lorenz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MK_h8wF0Rr4"
      },
      "source": [
        "Now lets download the training and validation data for the lorenz system. Info on wget from [Google drive](https://stackoverflow.com/questions/37453841/download-a-file-from-google-drive-using-wget). This will eventually be update to zenodo repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NtZ02zD0EKo"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU702uo6xIQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408f9d2f-7e79-4840-88b5-b7a7fb24de5b"
      },
      "source": [
        "!wget -O ./data/lorenz_training_rk.hdf5 \"https://drive.google.com/uc?export=download&id=1vGTGzaqEZxxuLN9K-PUrYw9SLWttdDYd\"\n",
        "!wget -O ./data/lorenz_valid_rk.hdf5 \"https://drive.google.com/uc?export=download&id=1bxFzKg8tSagE8kXWGm2mtaJ4gPsKJ8sI\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-28 16:43:26--  https://drive.google.com/uc?export=download&id=1vGTGzaqEZxxuLN9K-PUrYw9SLWttdDYd\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.73.206, 2607:f8b0:4004:837::200e\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.73.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0o-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/74vbh303qf81opqa4fp062e7qth6ub9d/1624898550000/01559412990587423567/*/1vGTGzaqEZxxuLN9K-PUrYw9SLWttdDYd?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-06-28 16:43:26--  https://doc-0o-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/74vbh303qf81opqa4fp062e7qth6ub9d/1624898550000/01559412990587423567/*/1vGTGzaqEZxxuLN9K-PUrYw9SLWttdDYd?e=download\n",
            "Resolving doc-0o-0o-docs.googleusercontent.com (doc-0o-0o-docs.googleusercontent.com)... 172.217.9.193, 2607:f8b0:4004:806::2001\n",
            "Connecting to doc-0o-0o-docs.googleusercontent.com (doc-0o-0o-docs.googleusercontent.com)|172.217.9.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-hdf]\n",
            "Saving to: ‘./data/lorenz_training_rk.hdf5’\n",
            "\n",
            "./data/lorenz_train     [   <=>              ]  12.78M  30.2MB/s    in 0.4s    \n",
            "\n",
            "2021-06-28 16:43:27 (30.2 MB/s) - ‘./data/lorenz_training_rk.hdf5’ saved [13403392]\n",
            "\n",
            "--2021-06-28 16:43:27--  https://drive.google.com/uc?export=download&id=1bxFzKg8tSagE8kXWGm2mtaJ4gPsKJ8sI\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.33.206, 2607:f8b0:4004:837::200e\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.33.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qes516d8efeifu0nh53r84l6775balem/1624898550000/01559412990587423567/*/1bxFzKg8tSagE8kXWGm2mtaJ4gPsKJ8sI?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-06-28 16:43:27--  https://doc-14-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qes516d8efeifu0nh53r84l6775balem/1624898550000/01559412990587423567/*/1bxFzKg8tSagE8kXWGm2mtaJ4gPsKJ8sI?e=download\n",
            "Resolving doc-14-0o-docs.googleusercontent.com (doc-14-0o-docs.googleusercontent.com)... 172.217.9.193, 2607:f8b0:4004:806::2001\n",
            "Connecting to doc-14-0o-docs.googleusercontent.com (doc-14-0o-docs.googleusercontent.com)|172.217.9.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1598976 (1.5M) [application/x-hdf]\n",
            "Saving to: ‘./data/lorenz_valid_rk.hdf5’\n",
            "\n",
            "./data/lorenz_valid 100%[===================>]   1.52M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-06-28 16:43:28 (17.7 MB/s) - ‘./data/lorenz_valid_rk.hdf5’ saved [1598976/1598976]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCHiYnrdZN95"
      },
      "source": [
        "# Transformer-PhysX Lorenz System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDVeeJHn11Ir"
      },
      "source": [
        "Train the embedding model.\n",
        "First import necessary modules from trphysx. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNGVZQ-o1gsH"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append('../..')\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "from trphysx.config.configuration_auto import AutoPhysConfig\n",
        "from trphysx.embedding.embedding_auto import AutoEmbeddingModel\n",
        "from trphysx.embedding.training import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEft0ltg4swx"
      },
      "source": [
        "Training arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R8QQ0cj4qR9"
      },
      "source": [
        "argv = []\n",
        "argv = argv + [\"--exp_name\", \"lorenz\"]\n",
        "argv = argv + [\"--training_h5_file\", \"./data/lorenz_training_rk.hdf5\"]\n",
        "argv = argv + [\"--eval_h5_file\", \"./data/lorenz_valid_rk.hdf5\"]\n",
        "argv = argv + [\"--batch_size\", '512']\n",
        "argv = argv + [\"--block_size\", \"16\"]\n",
        "argv = argv + [\"--ntrain\", \"2048\"]\n",
        "argv = argv + [\"--epochs\", \"100\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcBfHEh540f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0396769-de50-4f37-e491-eccd7c9eaf52"
      },
      "source": [
        "args = EmbeddingParser().parse(args=argv)  \n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    use_cuda = \"cuda\"\n",
        "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Torch device:{}\".format(args.device))\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch device:cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjxiqD3lF98_"
      },
      "source": [
        "Now we can use the auto classes to initialized the predefined configs, dataloaders and models. This may take a bit!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Fel9vqZVsH"
      },
      "source": [
        "## Initalizing Datasets and Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcpC9Fy243RN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc5a6e82-8692-4712-adc3-ded5b0f417cc"
      },
      "source": [
        " # Load transformer config file\n",
        "config = AutoPhysConfig.load_config(args.exp_name)\n",
        "dataloader = AutoDataHandler.load_data_handler(args.exp_name)\n",
        "\n",
        "# Set up data-loaders\n",
        "training_loader = dataloader.createTrainingLoader(args.training_h5_file, block_size=args.block_size, stride=args.stride, ndata=args.ntrain, batch_size=args.batch_size)\n",
        "testing_loader = dataloader.createTestingLoader(args.eval_h5_file, block_size=32, ndata=args.ntest, batch_size=8)\n",
        "\n",
        "# Set up model\n",
        "model = AutoEmbeddingModel.init_trainer(args.exp_name, config).to(args.device)\n",
        "mu, std = dataloader.norm_params\n",
        "model.embedding_model.mu = mu.to(args.device)\n",
        "model.embedding_model.std = std.to(args.device)\n",
        "if args.epoch_start > 1:\n",
        "  model.load_model(args.ckpt_dir, args.epoch_start)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/28/2021 16:49:01 - INFO - trphysx.embedding.training.enn_data_handler -   Creating training loader.\n",
            "06/28/2021 16:49:53 - INFO - trphysx.embedding.training.enn_data_handler -   Creating testing loader\n",
            "06/28/2021 16:49:55 - INFO - trphysx.embedding.embedding_lorenz -   Number of embedding parameters: 36192\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auDiMVZ5UfNz"
      },
      "source": [
        "Initialize optimizer and scheduler. Feel free to change if you want to experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnrtuKdhGuWQ"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr*0.995**(args.epoch_start-1), weight_decay=1e-8)\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.995)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2XPKfYTUuXf"
      },
      "source": [
        "Train the model. No visualization here, just boring numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It_LLGQIZe0b"
      },
      "source": [
        "## Training the Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ic9DFQcUWpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3dd8e00-eb41-4ed7-e9f3-8a8e3300df72"
      },
      "source": [
        "trainer = EmbeddingTrainer(model, args, (optimizer, scheduler))\n",
        "trainer.trainKoopman(training_loader, testing_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/28/2021 16:50:12 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 1: Training loss 40638908.000, Lr 0.00100\n",
            "06/28/2021 16:50:12 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 1: Test loss: 0.49\n",
            "06/28/2021 16:50:15 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 2: Training loss 811423.938, Lr 0.00099\n",
            "06/28/2021 16:50:17 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 3: Training loss 810006.938, Lr 0.00099\n",
            "06/28/2021 16:50:20 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 4: Training loss 681831.188, Lr 0.00099\n",
            "06/28/2021 16:50:23 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 5: Training loss 704575.250, Lr 0.00098\n",
            "06/28/2021 16:50:23 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 5: Test loss: 0.43\n",
            "06/28/2021 16:50:25 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 6: Training loss 391986.812, Lr 0.00098\n",
            "06/28/2021 16:50:28 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 7: Training loss 335938.438, Lr 0.00097\n",
            "06/28/2021 16:50:30 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 8: Training loss 299387.219, Lr 0.00097\n",
            "06/28/2021 16:50:33 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 9: Training loss 371889.594, Lr 0.00096\n",
            "06/28/2021 16:50:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 10: Training loss 627809.438, Lr 0.00096\n",
            "06/28/2021 16:50:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 10: Test loss: 0.42\n",
            "06/28/2021 16:50:38 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 11: Training loss 338897.656, Lr 0.00095\n",
            "06/28/2021 16:50:41 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 12: Training loss 246385.188, Lr 0.00095\n",
            "06/28/2021 16:50:43 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 13: Training loss 203537.875, Lr 0.00094\n",
            "06/28/2021 16:50:46 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 14: Training loss 320440.219, Lr 0.00094\n",
            "06/28/2021 16:50:48 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 15: Training loss 266844.688, Lr 0.00093\n",
            "06/28/2021 16:50:48 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 15: Test loss: 0.45\n",
            "06/28/2021 16:50:51 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 16: Training loss 276624.188, Lr 0.00093\n",
            "06/28/2021 16:50:53 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 17: Training loss 214417.344, Lr 0.00092\n",
            "06/28/2021 16:50:56 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 18: Training loss 199175.312, Lr 0.00092\n",
            "06/28/2021 16:50:59 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 19: Training loss 174553.547, Lr 0.00091\n",
            "06/28/2021 16:51:01 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 20: Training loss 153642.875, Lr 0.00091\n",
            "06/28/2021 16:51:01 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 20: Test loss: 0.41\n",
            "06/28/2021 16:51:04 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 21: Training loss 137634.703, Lr 0.00090\n",
            "06/28/2021 16:51:06 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 22: Training loss 125674.508, Lr 0.00090\n",
            "06/28/2021 16:51:09 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 23: Training loss 114345.289, Lr 0.00090\n",
            "06/28/2021 16:51:12 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 24: Training loss 107675.398, Lr 0.00089\n",
            "06/28/2021 16:51:14 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 25: Training loss 246420.938, Lr 0.00089\n",
            "06/28/2021 16:51:14 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 25: Test loss: 0.39\n",
            "06/28/2021 16:51:14 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
            "06/28/2021 16:51:17 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 26: Training loss 187434.297, Lr 0.00088\n",
            "06/28/2021 16:51:19 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 27: Training loss 149596.531, Lr 0.00088\n",
            "06/28/2021 16:51:22 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 28: Training loss 246384.234, Lr 0.00087\n",
            "06/28/2021 16:51:25 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 29: Training loss 187304.062, Lr 0.00087\n",
            "06/28/2021 16:51:27 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 30: Training loss 158069.656, Lr 0.00086\n",
            "06/28/2021 16:51:27 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 30: Test loss: 0.40\n",
            "06/28/2021 16:51:30 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 31: Training loss 140786.094, Lr 0.00086\n",
            "06/28/2021 16:51:33 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 32: Training loss 130467.672, Lr 0.00086\n",
            "06/28/2021 16:51:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 33: Training loss 119053.016, Lr 0.00085\n",
            "06/28/2021 16:51:38 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 34: Training loss 123270.125, Lr 0.00085\n",
            "06/28/2021 16:51:40 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 35: Training loss 116795.312, Lr 0.00084\n",
            "06/28/2021 16:51:40 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 35: Test loss: 0.40\n",
            "06/28/2021 16:51:43 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 36: Training loss 119434.359, Lr 0.00084\n",
            "06/28/2021 16:51:45 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 37: Training loss 186012.578, Lr 0.00083\n",
            "06/28/2021 16:51:48 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 38: Training loss 130815.336, Lr 0.00083\n",
            "06/28/2021 16:51:51 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 39: Training loss 182511.922, Lr 0.00083\n",
            "06/28/2021 16:51:53 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 40: Training loss 118636.188, Lr 0.00082\n",
            "06/28/2021 16:51:53 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 40: Test loss: 0.39\n",
            "06/28/2021 16:51:56 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 41: Training loss 100216.648, Lr 0.00082\n",
            "06/28/2021 16:51:58 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 42: Training loss 89692.727, Lr 0.00081\n",
            "06/28/2021 16:52:01 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 43: Training loss 84153.898, Lr 0.00081\n",
            "06/28/2021 16:52:03 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 44: Training loss 77335.586, Lr 0.00081\n",
            "06/28/2021 16:52:06 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 45: Training loss 70657.578, Lr 0.00080\n",
            "06/28/2021 16:52:06 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 45: Test loss: 0.39\n",
            "06/28/2021 16:52:09 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 46: Training loss 65869.734, Lr 0.00080\n",
            "06/28/2021 16:52:11 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 47: Training loss 62569.551, Lr 0.00079\n",
            "06/28/2021 16:52:14 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 48: Training loss 128662.055, Lr 0.00079\n",
            "06/28/2021 16:52:17 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 49: Training loss 112655.766, Lr 0.00079\n",
            "06/28/2021 16:52:19 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 50: Training loss 104878.336, Lr 0.00078\n",
            "06/28/2021 16:52:19 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 50: Test loss: 0.43\n",
            "06/28/2021 16:52:19 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
            "06/28/2021 16:52:22 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 51: Training loss 96263.922, Lr 0.00078\n",
            "06/28/2021 16:52:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 52: Training loss 91334.695, Lr 0.00077\n",
            "06/28/2021 16:52:27 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 53: Training loss 91044.648, Lr 0.00077\n",
            "06/28/2021 16:52:30 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 54: Training loss 84044.148, Lr 0.00077\n",
            "06/28/2021 16:52:32 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 55: Training loss 88296.203, Lr 0.00076\n",
            "06/28/2021 16:52:32 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 55: Test loss: 0.42\n",
            "06/28/2021 16:52:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 56: Training loss 95998.539, Lr 0.00076\n",
            "06/28/2021 16:52:37 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 57: Training loss 76922.266, Lr 0.00076\n",
            "06/28/2021 16:52:40 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 58: Training loss 71336.492, Lr 0.00075\n",
            "06/28/2021 16:52:42 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 59: Training loss 67939.898, Lr 0.00075\n",
            "06/28/2021 16:52:45 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 60: Training loss 68422.500, Lr 0.00074\n",
            "06/28/2021 16:52:45 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 60: Test loss: 0.42\n",
            "06/28/2021 16:52:48 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 61: Training loss 65119.508, Lr 0.00074\n",
            "06/28/2021 16:52:50 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 62: Training loss 71238.688, Lr 0.00074\n",
            "06/28/2021 16:52:53 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 63: Training loss 170615.078, Lr 0.00073\n",
            "06/28/2021 16:52:55 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 64: Training loss 72748.445, Lr 0.00073\n",
            "06/28/2021 16:52:58 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 65: Training loss 55268.332, Lr 0.00073\n",
            "06/28/2021 16:52:58 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 65: Test loss: 0.41\n",
            "06/28/2021 16:53:01 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 66: Training loss 49105.473, Lr 0.00072\n",
            "06/28/2021 16:53:03 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 67: Training loss 44790.059, Lr 0.00072\n",
            "06/28/2021 16:53:06 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 68: Training loss 43358.629, Lr 0.00071\n",
            "06/28/2021 16:53:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 69: Training loss 61688.879, Lr 0.00071\n",
            "06/28/2021 16:53:11 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 70: Training loss 76078.688, Lr 0.00071\n",
            "06/28/2021 16:53:11 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 70: Test loss: 0.41\n",
            "06/28/2021 16:53:13 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 71: Training loss 68000.695, Lr 0.00070\n",
            "06/28/2021 16:53:16 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 72: Training loss 60436.602, Lr 0.00070\n",
            "06/28/2021 16:53:19 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 73: Training loss 54903.027, Lr 0.00070\n",
            "06/28/2021 16:53:21 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 74: Training loss 49420.477, Lr 0.00069\n",
            "06/28/2021 16:53:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 75: Training loss 45960.395, Lr 0.00069\n",
            "06/28/2021 16:53:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 75: Test loss: 0.40\n",
            "06/28/2021 16:53:24 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
            "06/28/2021 16:53:26 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 76: Training loss 98895.828, Lr 0.00069\n",
            "06/28/2021 16:53:29 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 77: Training loss 71740.578, Lr 0.00068\n",
            "06/28/2021 16:53:32 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 78: Training loss 67136.938, Lr 0.00068\n",
            "06/28/2021 16:53:34 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 79: Training loss 58135.984, Lr 0.00068\n",
            "06/28/2021 16:53:37 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 80: Training loss 57751.434, Lr 0.00067\n",
            "06/28/2021 16:53:37 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 80: Test loss: 0.43\n",
            "06/28/2021 16:53:39 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 81: Training loss 53674.543, Lr 0.00067\n",
            "06/28/2021 16:53:42 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 82: Training loss 50433.602, Lr 0.00067\n",
            "06/28/2021 16:53:45 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 83: Training loss 48559.949, Lr 0.00066\n",
            "06/28/2021 16:53:47 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 84: Training loss 47183.879, Lr 0.00066\n",
            "06/28/2021 16:53:50 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 85: Training loss 45102.141, Lr 0.00066\n",
            "06/28/2021 16:53:50 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 85: Test loss: 0.43\n",
            "06/28/2021 16:53:52 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 86: Training loss 47524.324, Lr 0.00065\n",
            "06/28/2021 16:53:55 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 87: Training loss 44554.934, Lr 0.00065\n",
            "06/28/2021 16:53:57 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 88: Training loss 45955.008, Lr 0.00065\n",
            "06/28/2021 16:54:00 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 89: Training loss 50564.586, Lr 0.00064\n",
            "06/28/2021 16:54:03 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 90: Training loss 60709.785, Lr 0.00064\n",
            "06/28/2021 16:54:03 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 90: Test loss: 0.40\n",
            "06/28/2021 16:54:05 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 91: Training loss 57780.000, Lr 0.00064\n",
            "06/28/2021 16:54:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 92: Training loss 54836.570, Lr 0.00063\n",
            "06/28/2021 16:54:10 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 93: Training loss 53824.199, Lr 0.00063\n",
            "06/28/2021 16:54:13 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 94: Training loss 48533.496, Lr 0.00063\n",
            "06/28/2021 16:54:15 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 95: Training loss 35640.203, Lr 0.00062\n",
            "06/28/2021 16:54:15 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 95: Test loss: 0.41\n",
            "06/28/2021 16:54:18 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 96: Training loss 34635.316, Lr 0.00062\n",
            "06/28/2021 16:54:21 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 97: Training loss 34221.969, Lr 0.00062\n",
            "06/28/2021 16:54:23 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 98: Training loss 33033.863, Lr 0.00061\n",
            "06/28/2021 16:54:26 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 99: Training loss 76787.625, Lr 0.00061\n",
            "06/28/2021 16:54:28 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 100: Training loss 43286.730, Lr 0.00061\n",
            "06/28/2021 16:54:28 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 100: Test loss: 0.39\n",
            "06/28/2021 16:54:28 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3ZEE4ixh8nr"
      },
      "source": [
        "Check your Google drive for checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7J5GgEFh_8t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}